{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Install dependencies\n"
      ],
      "metadata": {
        "id": "-Y0H7EwLc7-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkfxWcNVb6sl"
      },
      "outputs": [],
      "source": [
        "!pip install torch numpy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the ***Four in a Row*** Environment"
      ],
      "metadata": {
        "id": "dTFycDVddIix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of rows and columns in the game\n",
        "ROWS = 6\n",
        "COLS = 7\n",
        "\n",
        "class ConnectFourEnv:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((ROWS, COLS), dtype=int)  # 6x7 grid with zeros.\n",
        "        self.done = False   # is game over?\n",
        "        self.winner = None  # stores winner (AI:1, Human:2)\n",
        "\n",
        "    def reset(self):    # reset game before new game\n",
        "        self.board = np.zeros((ROWS, COLS), dtype=int)\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.board\n",
        "\n",
        "    def step(self, col, player):    # drop piece into selected column\n",
        "        if self.done or self.board[0][col] != 0:    # column is full or game over\n",
        "            return self.board, -10, True  # invalid move = penalty\n",
        "\n",
        "        for row in range(ROWS-1, -1, -1):\n",
        "            if self.board[row][col] == 0:\n",
        "                self.board[row][col] = player\n",
        "                break\n",
        "\n",
        "        if self.check_win(player):  # check if the move wins the game\n",
        "            self.done = True\n",
        "            self.winner = player\n",
        "            return self.board, 1 if player == 1 else -1, True   # rewarding/punishment\n",
        "\n",
        "        if not any(0 in row for row in self.board): # if the board is full, game over with draw\n",
        "            self.done = True\n",
        "            return self.board, 0, True  # no reward/punishment\n",
        "\n",
        "        return self.board, 0, False # otherwise, continue game\n",
        "\n",
        "    def check_win(self, player):\n",
        "        # horizontal check\n",
        "        for r in range(ROWS):\n",
        "            for c in range(COLS-3):\n",
        "                if all(self.board[r, c+i] == player for i in range(4)):\n",
        "                    return True\n",
        "\n",
        "        # vertical check\n",
        "        for r in range(ROWS-3):\n",
        "            for c in range(COLS):\n",
        "                if all(self.board[r+i, c] == player for i in range(4)):\n",
        "                    return True\n",
        "\n",
        "        # diagonal check\n",
        "        for r in range(ROWS-3):\n",
        "            for c in range(COLS-3):\n",
        "                if all(self.board[r+i, c+i] == player for i in range(4)) or \\\n",
        "                   all(self.board[r+3-i, c+i] == player for i in range(4)):\n",
        "                    return True\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "b8sNhZfQcxPH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-Based Deep Q-Network (DQN)"
      ],
      "metadata": {
        "id": "1uCSdSeVdbTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, COLS)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "tNIcHVfkddfY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experience Replay Memory"
      ],
      "metadata": {
        "id": "SLDRjJyhd0Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "    # store 10.000 experience\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.memory = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def push(self, experience):\n",
        "        if len(self.memory) > self.capacity:\n",
        "            self.memory.pop(0)\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "4B3DiFUTd1vW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the CNN-DQN Model"
      ],
      "metadata": {
        "id": "1dc_H7wRd76l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cnn_dqn():\n",
        "    env = ConnectFourEnv()\n",
        "    dqn = CNN_DQN()\n",
        "    optimizer = optim.Adam(dqn.parameters(), lr=0.001)\n",
        "    memory = ReplayMemory()\n",
        "\n",
        "    episodes = 5000\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.9995\n",
        "    epsilon_min = 0.1\n",
        "    batch_size = 64\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice([c for c in range(COLS) if env.board[0][c] == 0])\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(dqn(state)).item()  # Exploit\n",
        "\n",
        "            next_state, reward, done = env.step(action, 1)\n",
        "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "            memory.push((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "\n",
        "            if len(memory) > batch_size:\n",
        "                batch = memory.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "                states = torch.cat(states)\n",
        "                actions = torch.tensor(actions).unsqueeze(1)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                next_states = torch.cat(next_states)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                q_values = dqn(states).gather(1, actions).squeeze()\n",
        "                next_q_values = dqn(next_states).max(1)[0].detach()\n",
        "                target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "                loss = F.mse_loss(q_values, target_q_values)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        if episode % 500 == 0:\n",
        "            print(f\"Episode {episode}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    torch.save(dqn.state_dict(), \"cnn_dqn.pth\")\n",
        "    print(\"Training complete, model saved.\")\n",
        "\n",
        "train_cnn_dqn()\n"
      ],
      "metadata": {
        "id": "ManSXRNld9ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_cnn_dqn():\n",
        "    env = ConnectFourEnv()\n",
        "    dqn = CNN_DQN()\n",
        "    dqn.load_state_dict(torch.load(\"cnn_dqn.pth\"))\n",
        "    dqn.eval()\n",
        "\n",
        "    state = env.reset()\n",
        "    print(\"You are Player 2 (O), AI is Player 1 (X)\")\n",
        "\n",
        "    while not env.done:\n",
        "        print(np.flip(env.board, 0))\n",
        "        move = int(input(\"Input your piece to column (0-6): \"))\n",
        "        env.step(move, 2)\n",
        "\n",
        "        if not env.done:\n",
        "            state_tensor = torch.tensor(env.board, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "            ai_move = torch.argmax(dqn(state_tensor)).item()\n",
        "            env.step(ai_move, 1)\n",
        "            print(f\"AI played column {ai_move}\")\n",
        "\n",
        "    print(np.flip(env.board, 0))\n",
        "    print(\"Game Over!\")\n",
        "\n",
        "play_cnn_dqn()\n"
      ],
      "metadata": {
        "id": "10ehOcHXhy5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}